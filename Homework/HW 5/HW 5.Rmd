---
title: "HW 5"
author: "Lynn Check"
date: "11/08/2024"
output:
  pdf_document: default
  html_document:
    number_sections: true
---

**This homework is meant to give you practice in creating and defending a position with both statistical and philosophical evidence.  We have now extensively talked about the COMPAS ^[https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis] data set, the flaws in applying it but also its potential upside if its shortcomings can be overlooked.  We have also spent time in class verbally assessing positions both for an against applying this data set in real life.  In no more than two pages ^[knit to a pdf to ensure page count] take the persona of a statistical consultant advising a judge as to whether they should include the results of the COMPAS algorithm in their decision making process for granting parole.  First clearly articulate your position (whether the algorithm should be used or not) and then defend said position using both statistical and philosophical evidence.  Your paper will be grade both on the merits of its persuasive appeal but also the applicability of the statistical and philosophical evidence cited.**



Subject: Recommendation of the Usage of the COMPAS Algorithm in Parole Decision-Making Process

Your Honor, 

As an experienced statistical consultant advising on tools that impact judicial decisions, I strongly advise against implementing the COMPAS algorithm into the parole decision making process. My recommendation stems from the racial biases—especially in the disproportionate false positives for Black defendants—the opacity and lack of transparency inherent in its black box design, its perpetuation of systemic biases through the reliance on historically biased data, statistical fairness, and ethical concerns. 

A critical concern regarding arbitrary biases with the COMPAS algorithm is demonstrated by the disproportionate representation between Black and White defendants. Despite the algorithm being designed to assess recidivism risk objectively, there is a disproportionately high number of false positives amongst the Black defendants compared to the White defendants. What this means is that Black individuals are more likely to be identified as high risk for reoffending (45%) than White individuals (23%). Nearly half of the Black defendants who were non-recidivists were unjustly evaluated as likely to recidivate while of those who were reoffenders 48% of the White individuals were incorrectly classified as low risk to reoffend (false negatives). The alarming disparity originates from the algorithm’s reliance on factors that are proxies for race, such as prior arrests, socioeconomic status, neighborhood demographics, and employment which are all historically influenced by systematic inequalities. Incorporating the COMPAS algorithm would further exacerbate the existing racial inequalities within the criminal justice system. 

The disproportionality is not the only concerning aspect to COMPAS. Independent studies conducted on the algorithm reported an accuracy rate of 61% to 65% despite Northpointe’s, the company selling the algorithm, reported accuracy of the algorithm being 68%. The difference in accuracy percentage is marginal, giving rise to concerns about the fairness and equity of the overall accuracy and how errors are distributed among different demographic groups. However, there are still elements that these independent studies could not investigate due to COMPAS operating as a “black box”. The term “black box” refers to its proprietary nature where Northpointe, the company selling the algorithm for profit, did not disclose any specific variables, decision rules and parameters used in the algorithm, posing serious issues due to the lack of transparency. Without access to the methodology of the algorithm, the algorithm can not be subjected to critical evaluation and scrutinization by experts for biases and errors, preventing identification and correction to potential flaws that could lead to unfair outcomes. It also prevents defendants the ability to challenge the accuracy or fairness of their risk assessment, stripping them of their right to a fair hearing. 

To further emphasize on the perpetuation of systemic biases, the design and application of COMPAS reinforces the patterns of discrimination because of its usage of data that reflects historical inequalities. The algorithm incorporates criminal history and arrest records which are influenced by over-policing in minority communities. This directly affects individuals from these communities by placing them at a disadvantage with the algorithm, projecting them to be higher risk scorers, creating a negative feedback loop of bias. The projection would not necessarily be based on their likelihood to reoffend, but rather because they have been disproportionately targeted by law enforcement. The algorithm also fails to consider individual circumstances by its strong reliance on group data. The lack of individualized assessment can unjustly hinder opportunities for parole, especially for those who have shown significant self-growth or have a strong support system in place, ultimately enabling a cycle where disadvantaged groups continue to face penalties and fewer opportunities for reintegration into society. Furthermore, the discrepancies of the algorithm directly violate the principle of justice as fairness, articulated by philosopher John Rawls, emphasizing that societal institutions should ensure fairness and equality. Rawls advocated for society to be structured under two fundamental principles of justice: the Principle of Equal Basic Liberties, the Difference Principle and Fair Equality of Opportunity. The difference principle places emphasis on the justification for social and economic inequalities being justifiable when it is the result of allocating resources to the least advantageous members of society. However, instead of protecting the minorities, COMPAS is further contributing to injustice and prejudice against Black individuals. I want to emphasize that it is the duty of the American Justice System to ensure that the jurisdiction made by the Judge is one made “fairly, impartially and diligently” as stated in the Code of Conduct. Therefore, it is crucial we consider every possible statistical and ethical implication of when COMPAS can stray away from making an accurate and just assessment for individuals. Allowing the influence of COMPAS in making parole decisions would subject individuals to bearing the consequences of the algorithm’s error, especially for the Black community. Their social background should not dictate the outcome of their future, supported by the Fair Equality of Opportunity principle. 

It is important to note that humans can be subconsciously biased which then influences their decisions and COMPAS can assist with the mitigation of that said subconscious biases. However, COMPAS, as a computational algorithm is demonstrating a higher likelihood of incorrectly labeling the recidivism of individuals. To reiterate, the disproportionately high false positives observed with Black individuals and false negatives with Whites individuals illustrate the stark racial discrimination to the innate structure of COMPAS. It would not only violate the Code of Conduct for the United States Judges, but it would also be unfair to every individual that stands before the Judge to be reduced down to an assessment made by an algorithm trained on a group dataset.  In short I feel that you should not consider the use of COMPAS as a part of the parole decision-making process. Thank you for considering all the possible implications regarding COMPAS and seeking a second perspective to the usage of COMPAS in the parole decision making process.


Best, 

Lynn Check

